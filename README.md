# Natural Language Processing

# Main Topics Learned

**Language Structure** - Part of speech tagging, hidden Markov model (HMM), Bayes theorem, Bag of Words

**Math for Topic Modelling** - GLoVe, Word2vec, Latent Dirichlet allocation (LDA), Dirichlet distributions (alpha-beta), matrix factorization

**RNN Networks** - simple RNN, Bidirectional RNN, Sequence to Sequence, LSTM, Additive Attention -> Multiplicative Attention -> Self-Attention

**Verbal Communication** - Voice User Interface

# Resources

### Textbook



### Paper

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
- [Attention is all You Need](https://arxiv.org/abs/1706.03762)


### Structured

The Udacity Nanodegree program provided great assistance. I really like their approach of starting from the high level understanding of say Attention, then going a stage lower video by video. Most importantly, it provided me with pretty good GPU (Tesla K80).